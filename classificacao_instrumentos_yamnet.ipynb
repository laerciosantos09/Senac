from nbformat import v4 as nbf
import json

# Cria√ß√£o do notebook
notebook = nbf.new_notebook()

# Lista de blocos de c√≥digo (cada c√©lula separada)
cells = []

# Bloco 1: Instala√ß√£o
cells.append(nbf.new_code_cell("""\
# üì¶ Instala√ß√£o de bibliotecas
!pip install kaggle tensorflow tensorflow_hub librosa matplotlib seaborn tqdm"""))

# Bloco 2: Diret√≥rios e API do Kaggle
cells.append(nbf.new_code_cell("""\
# üìÅ Configura√ß√£o de diret√≥rios
import os
os.makedirs('/content/kaggle', exist_ok=True)
os.makedirs('/root/.kaggle', exist_ok=True)

# üîë Baixando kaggle.json do Google Drive
import requests
file_id = "1drlyKgvfvFaE69SzowRjcF5KK14jaroQ"
url = f"https://drive.google.com/uc?export=download&id={file_id}"
dest_path = "/content/kaggle/kaggle.json"
response = requests.get(url)
with open(dest_path, "wb") as f:
    f.write(response.content)
!cp "{dest_path}" "/root/.kaggle"
!chmod 600 /root/.kaggle/kaggle.json"""))

# Bloco 3: Download do dataset
cells.append(nbf.new_code_cell("""\
# üì• Download e descompacta√ß√£o do dataset
DATA_DIR = '/content/musical'
DATASET_SLUG = 'soumendraprasad/musical-instruments-sound-dataset'
ZIP_FILE = os.path.join(DATA_DIR, 'musical-instruments-sound-dataset.zip')
os.makedirs(DATA_DIR, exist_ok=True)
!kaggle datasets download -d {DATASET_SLUG} -p {DATA_DIR}
!unzip -q {ZIP_FILE} -d {DATA_DIR}"""))

# Bloco 4: An√°lise explorat√≥ria
cells.append(nbf.new_code_cell("""\
# üîç An√°lise Explorat√≥ria
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.io import wavfile
import librosa
import librosa.display

df = pd.read_csv(f"{DATA_DIR}/Metadata_Train.csv")
df2 = pd.read_csv(f"{DATA_DIR}/Metadata_Test.csv")

print(df['Class'].value_counts())
plt.figure(figsize=(22,10))
sns.countplot(df['Class'])
plt.xticks(rotation=0)
plt.show()"""))

# Bloco 5: Dura√ß√£o e distribui√ß√£o
cells.append(nbf.new_code_cell("""\
df.set_index('FileName', inplace=True)
for f in df.index:
    rate, signal = wavfile.read(f"{DATA_DIR}/Train_submission/Train_submission/" + f)
    df.at[f, 'length'] = signal.shape[0]/rate

classes = list(np.unique(df.Class))
class_dist = df.groupby(['Class'])['length'].mean()

fig, ax = plt.subplots()
ax.set_title('Distribui√ß√£o por Classe', y=1.08)
ax.pie(class_dist, labels=class_dist.index, autopct='%1.1f%%', shadow=False, startangle=90)
ax.axis('equal')
plt.show()
df.reset_index(inplace=True)"""))

# Bloco 6: FFT e envelope
cells.append(nbf.new_code_cell("""\
def calc_fft(y, sr):
    n = len(y)
    freq = np.fft.rfftfreq(n, d=1/sr)
    Y = abs(np.fft.rfft(y)/n)
    return (Y, freq)

def Envelope(y, rate, threshold):
    y = pd.Series(y).apply(np.abs)
    y_mean = y.rolling(window=int(rate/4), min_periods=1, center=True).mean()
    return y_mean > threshold

signals = {}
fft = {}
fnames = []

for c in classes:
    wav_file = df[df.Class == c].iloc[0, 0]
    fnames.append(f"{DATA_DIR}/Train_submission/Train_submission/" + wav_file)
    signal, rate = librosa.load(fnames[-1], sr=44100)
    mask = Envelope(signal, rate, 0.0005)
    signal = signal[mask]
    signals[c] = signal
    fft[c] = calc_fft(signal, rate)"""))

# Bloco 7: Visualiza√ß√£o de sinais e FFTs
cells.append(nbf.new_code_cell("""\
def plot_signals(signals):
    fig, axes = plt.subplots(1, 4, figsize=(20,5))
    fig.suptitle('Sinais Temporais', size=16)
    for i, (label, sig) in enumerate(signals.items()):
        axes[i].set_title(label)
        axes[i].plot(sig)
        axes[i].axis('off')
plot_signals(signals)
plt.show()

def plot_fft(fft):
    fig, axes = plt.subplots(1, 4, figsize=(20,5))
    fig.suptitle('Transformadas de Fourier', size=16)
    for i, (label, data) in enumerate(fft.items()):
        Y, freq = data
        axes[i].set_title(label)
        axes[i].plot(freq, Y)
        axes[i].axis('off')
plot_fft(fft)
plt.show()"""))

# Bloco 8: Espectrogramas Mel
cells.append(nbf.new_code_cell("""\
hop_length = 512
fig, ax = plt.subplots(1, 4, figsize=(16, 9))
fig.suptitle('Espectrogramas Mel', fontsize=16)

for i, fname in enumerate(fnames):
    y_data, sr_data = librosa.load(fname, sr=25000)
    S = librosa.feature.melspectrogram(y_data, sr=25000)
    S_db = librosa.amplitude_to_db(S, ref=np.max)
    librosa.display.specshow(S_db, sr=25000, hop_length=hop_length, x_axis='time', y_axis='log', cmap='rainbow', ax=ax[i])
    ax[i].set_title(classes[i], fontsize=13)
plt.show()"""))

# Bloco 9: Transfer learning com YAMNet
cells.append(nbf.new_code_cell("""\
import tensorflow_hub as hub
yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')

def preprocess_audio(file_path):
    waveform, sr = librosa.load(file_path, sr=16000, mono=True)
    waveform = waveform / np.max(np.abs(waveform))
    return waveform

def extract_embeddings(waveform):
    scores, embeddings, spectrogram = yamnet_model(waveform)
    return embeddings.numpy().mean(axis=0)"""))

# Bloco 10: Prepara√ß√£o dos dados
cells.append(nbf.new_code_cell("""\
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tqdm import tqdm

X = []
y = []

for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        waveform = preprocess_audio(f"{DATA_DIR}/Train_submission/Train_submission/" + row['FileName'])
        embedding = extract_embeddings(waveform)
        X.append(embedding)
        y.append(row['Class'])
    except Exception as e:
        print(f"Erro no arquivo {row['FileName']}: {e}")

X = np.array(X)
le = LabelEncoder()
y_encoded = le.fit_transform(y)
y_cat = to_categorical(y_encoded)

X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)"""))

# Bloco 11: Treinamento do classificador
cells.append(nbf.new_code_cell("""\
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential([
    Dense(128, activation='relu', input_shape=(1024,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(4, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))"""))

# Bloco 12: Avalia√ß√£o
cells.append(nbf.new_code_cell("""\
plt.plot(history.history['accuracy'], label='Treino')
plt.plot(history.history['val_accuracy'], label='Valida√ß√£o')
plt.title('Acur√°cia')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Treino')
plt.plot(history.history['val_loss'], label='Valida√ß√£o')
plt.title('Perda')
plt.legend()
plt.show()"""))

# Adiciona todas as c√©lulas ao notebook
notebook['cells'] = cells

# Salva como arquivo .ipynb
with open("classificacao_instrumentos_yamnet.ipynb", "w") as f:
    json.dump(notebook, f)

print("‚úÖ Notebook gerado: classificacao_instrumentos_yamnet.ipynb")
